# webcrawler
웹 크롤링 스크립트 
# web_crawler.py 스크립트 사용 설명서 (Readme)

## 1. 개요

이 스크립트는 지정된 웹사이트의 모든 페이지를 순회하며, 각 페이지의 텍스트 콘텐츠를 추출하여 별도의 .txt 파일로 저장하는 파이썬 웹 크롤러입니다.

---

## 2. 주요 기능

- **웹사이트 순회**: 시작 URL로부터 연결된 모든 내부 페이지를 재귀적으로 방문합니다.
- **텍스트 추출**: 각 페이지에서 스크립트(JavaScript), 스타일(CSS), 이미지 태그를 제외하고 순수 텍스트 정보만 추출합니다.
- **파일 저장**: 추출된 텍스트는 각 페이지별로 고유한 .txt 파일로 만들어 저장합니다. 파일명은 해당 페이지의 URL 경로를 기반으로 생성됩니다.
- **서버 부하 방지**: 각 페이지 요청 후 2초의 대기 시간을 두어 대상 서버에 과도한 부하를 주지 않도록 설계되었습니다.
- **실행 위치**: 스크립트 실행 시, 결과물은 아래 "5. 설정"에 명시된 경로에 저장됩니다.

---

## 3. 설치 및 준비

스크립트를 실행하기 전에, 아래의 Python 라이브러리들이 설치되어 있어야 합니다.
터미널 또는 명령 프롬프트에 다음 명령어를 입력하여 설치하세요.

```
pip install requests beautifulsoup4
```

---

## 4. 실행 방법

터미널 또는 명령 프롬프트에서 `web_crawler.py` 파일이 있는 경로로 이동하거나, 전체 경로를 지정하여 아래와 같이 실행합니다.

```
python C:\Users\GFS_User\web_crawler.py
```

스크립트가 실행되면 "크롤링 중..." 메시지와 함께 작업이 시작되며, 완료되면 "크롤링이 완료되었습니다." 메시지가 나타납니다.

---

## 5. 설정

스크립트 파일(`web_crawler.py`)을 직접 수정하여 일부 설정을 변경할 수 있습니다.

- **저장 위치 변경**:
  - `crawl_website` 함수 내의 `output_dir` 변수 값을 원하는 폴더 경로로 변경하세요.
  - 현재 설정: `output_dir = "C:/Users/GFS_User/Downloads/crawled_text"`

- **크롤링 대상 URL 변경**:
  - 스크립트 하단의 `if __name__ == "__main__":` 블록 안에 있는 `start_url` 변수 값을 원하는 웹사이트 주소로 변경하세요.
  - 현재 설정: `start_url = "https://stockstalker.co.kr/"`
